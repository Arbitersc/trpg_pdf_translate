# ================================================
# 环境变量配置文件（示例）
# ================================================
# 复制此文件为 .env 并填入实际的配置值
# 注意：.env 文件不应提交到版本控制系统
# ================================================

# ================================================
# 环境标识
# ================================================
# 环境类型: development | staging | production
ENVIRONMENT=development

# ================================================
# LLM 提供商配置
# ================================================
# 可用提供商: openai, ollama
# openai - 支持 OpenAI API 及兼容格式的 API（如 Azure OpenAI、硅基流动、智谱 AI 等）
# ollama  - 本地 Ollama 服务
LLM_PROVIDER=openai

# ================================================
# OpenAI 兼容配置
# ================================================
# API 密钥（必需）
# OpenAI 官方: https://platform.openai.com/api-keys
# 硅基流动: https://siliconflow.cn/account/ak
# 智谱 AI: https://open.bigmodel.cn/usercenter/apikeys
OPENAI_API_KEY=your-api-key-here

# API 基础 URL（可选，默认为 OpenAI 官方地址）
# OpenAI 官方: https://api.openai.com/v1
# 硅基流动: https://api.siliconflow.cn/v1
# 智谱 AI: https://open.bigmodel.cn/api/paas/v4
# Azure OpenAI: https://your-resource.openai.azure.com/openai/deployments/your-deployment
OPENAI_BASE_URL=https://api.openai.com/v1

# 模型名称（可选，有默认值）
# OpenAI: gpt-4o, gpt-4o-mini, gpt-3.5-turbo
# 硅基流动: Pro/zai-org/GLM-4.7, deepseek-ai/DeepSeek-V2.5
# 智谱 AI: glm-4, glm-4-flash, glm-4-plus
OPENAI_MODEL=gpt-4o-mini

# ================================================
# Ollama 配置
# ================================================
# Ollama 服务地址
OLLAMA_BASE_URL=http://localhost:11434

# Ollama 模型名称
OLLAMA_MODEL=llama3.2

# ================================================
# Flask 应用配置
# ================================================
# Flask 调试模式（生产环境应设为 false）
FLASK_DEBUG=true

# Flask 运行主机
FLASK_HOST=0.0.0.0

# Flask 运行端口
FLASK_PORT=5000

# ================================================
# PDF 存储配置
# ================================================
# PDF 文件存储目录（相对于 backend 目录）
PDF_DIRECTORY=pdfs
